{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "IN_COLAB = \"google.colab\" in sys.modules\n",
    "if IN_COLAB:\n",
    "    !pip install tianshou gym wandb"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tianshou as ts\n",
    "import gym\n",
    "from tianshou.utils.net.common import Net\n",
    "import tianshou.utils.net.discrete as discrete\n",
    "import torch\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from tianshou.utils import WandbLogger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\mambaforge\\envs\\rlenv\\lib\\site-packages\\notebook\\utils.py:280: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "  return LooseVersion(v) >= LooseVersion(check)\n",
      "\u001B[34m\u001B[1mwandb\u001B[0m: Currently logged in as: \u001B[33mdarthreca\u001B[0m. Use \u001B[1m`wandb login --relogin`\u001B[0m to force relogin\n",
      "D:\\mambaforge\\envs\\rlenv\\lib\\site-packages\\wandb\\sdk\\lib\\ipython.py:58: DeprecationWarning: Importing display from IPython.core.display is deprecated since IPython 7.14, please import from IPython display\n",
      "  from IPython.core.display import display\n"
     ]
    },
    {
     "data": {
      "text/plain": "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.016666666666666666, max=1.0â€¦",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "29872f9c5644423698266745d0402c2a"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "wandb version 0.13.9 is available!  To upgrade, please run:\n $ pip install wandb --upgrade"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Tracking run with wandb version 0.13.8"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Run data is saved locally in <code>D:\\Git Repos\\RL-Exercises\\wandb\\run-20230113_163933-gavtejs9</code>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Syncing run <strong><a href=\"https://wandb.ai/darthreca/tianshou/runs/gavtejs9\" target=\"_blank\">azure-field-2</a></strong> to <a href=\"https://wandb.ai/darthreca/tianshou\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": " View project at <a href=\"https://wandb.ai/darthreca/tianshou\" target=\"_blank\">https://wandb.ai/darthreca/tianshou</a>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": " View run at <a href=\"https://wandb.ai/darthreca/tianshou/runs/gavtejs9\" target=\"_blank\">https://wandb.ai/darthreca/tianshou/runs/gavtejs9</a>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\mambaforge\\envs\\rlenv\\lib\\site-packages\\wandb\\sdk\\lib\\import_hooks.py:246: DeprecationWarning: Deprecated since Python 3.4. Use importlib.util.find_spec() instead.\n",
      "  loader = importlib.find_loader(fullname, path)\n"
     ]
    }
   ],
   "source": [
    "logger = WandbLogger()\n",
    "writer = SummaryWriter(\"logs/lunar-lander\")\n",
    "logger.load(writer)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "env = gym.make(\"LunarLander-v2\")\n",
    "train_env = ts.env.DummyVectorEnv([lambda: gym.make(\"LunarLander-v2\") for _ in range(10)])\n",
    "test_env = ts.env.DummyVectorEnv([lambda: gym.make(\"LunarLander-v2\") for _ in range(2)])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [],
   "source": [
    "state_shape = env.observation_space.shape or env.observation_space.n\n",
    "actions_n = env.action_space.shape or env.action_space.n\n",
    "\n",
    "actor = discrete.Actor(Net(state_shape, hidden_sizes=[32, 32]), action_shape=actions_n)\n",
    "actor_optim = torch.optim.Adam(actor.parameters(), lr=3e-4)\n",
    "\n",
    "critic_1 = discrete.Critic(Net(state_shape, actions_n, hidden_sizes=[32, 32]), last_size=actions_n)\n",
    "critic_1_optim = torch.optim.Adam(critic_1.parameters(), lr=3e-4)\n",
    "\n",
    "critic_2 = discrete.Critic(Net(state_shape, actions_n, hidden_sizes=[32, 32]), last_size=actions_n)\n",
    "critic_2_optim = torch.optim.Adam(critic_2.parameters(), lr=3e-4)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "outputs": [
    {
     "data": {
      "text/plain": "Critic(\n  (preprocess): Net(\n    (model): MLP(\n      (model): Sequential(\n        (0): Linear(in_features=8, out_features=32, bias=True)\n        (1): ReLU()\n        (2): Linear(in_features=32, out_features=32, bias=True)\n        (3): ReLU()\n        (4): Linear(in_features=32, out_features=4, bias=True)\n      )\n    )\n  )\n  (last): MLP(\n    (model): Sequential(\n      (0): Linear(in_features=4, out_features=1, bias=True)\n    )\n  )\n)"
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "critic_1"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "outputs": [],
   "source": [
    "dqn = ts.policy.DiscreteSACPolicy(\n",
    "    actor, actor_optim,\n",
    "    critic_1, critic_1_optim,\n",
    "    critic_2, critic_2_optim,\n",
    "    tau=0.005, gamma=0.9,\n",
    ")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "outputs": [],
   "source": [
    "train_collector = ts.data.Collector(dqn, train_env, ts.data.VectorReplayBuffer(20000, 10), exploration_noise=True)\n",
    "test_collector = ts.data.Collector(dqn, test_env, exploration_noise=True)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [],
   "source": [
    "def train_fn(epoch, step):\n",
    "    pass"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #1:   0%|          | 10/10000 [00:00<00:16, 593.97it/s]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "index 1 is out of bounds for dimension 1 with size 1",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mRuntimeError\u001B[0m                              Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[31], line 1\u001B[0m\n\u001B[1;32m----> 1\u001B[0m \u001B[43mts\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtrainer\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43moffpolicy_trainer\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m      2\u001B[0m \u001B[43m    \u001B[49m\u001B[43mdqn\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtrain_collector\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtest_collector\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmax_epoch\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m10\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mbatch_size\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m128\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[0;32m      3\u001B[0m \u001B[43m    \u001B[49m\u001B[43mstep_per_epoch\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m10000\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mstep_per_collect\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m10\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[0;32m      4\u001B[0m \u001B[43m    \u001B[49m\u001B[43mtrain_fn\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtrain_fn\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m      5\u001B[0m \u001B[43m    \u001B[49m\u001B[43mepisode_per_test\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m10\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mstop_fn\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mlambda\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mrew\u001B[49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mrew\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m>\u001B[39;49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m \u001B[49m\u001B[43menv\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mspec\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mreward_threshold\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m      6\u001B[0m \u001B[43m    \u001B[49m\u001B[43mlogger\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mlogger\u001B[49m\n\u001B[0;32m      7\u001B[0m \u001B[43m)\u001B[49m\n",
      "File \u001B[1;32mD:\\mambaforge\\envs\\rlenv\\lib\\site-packages\\tianshou\\trainer\\offpolicy.py:133\u001B[0m, in \u001B[0;36moffpolicy_trainer\u001B[1;34m(*args, **kwargs)\u001B[0m\n\u001B[0;32m    126\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21moffpolicy_trainer\u001B[39m(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Dict[\u001B[38;5;28mstr\u001B[39m, Union[\u001B[38;5;28mfloat\u001B[39m, \u001B[38;5;28mstr\u001B[39m]]:  \u001B[38;5;66;03m# type: ignore\u001B[39;00m\n\u001B[0;32m    127\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"Wrapper for OffPolicyTrainer run method.\u001B[39;00m\n\u001B[0;32m    128\u001B[0m \n\u001B[0;32m    129\u001B[0m \u001B[38;5;124;03m    It is identical to ``OffpolicyTrainer(...).run()``.\u001B[39;00m\n\u001B[0;32m    130\u001B[0m \n\u001B[0;32m    131\u001B[0m \u001B[38;5;124;03m    :return: See :func:`~tianshou.trainer.gather_info`.\u001B[39;00m\n\u001B[0;32m    132\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[1;32m--> 133\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mOffpolicyTrainer\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrun\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32mD:\\mambaforge\\envs\\rlenv\\lib\\site-packages\\tianshou\\trainer\\base.py:441\u001B[0m, in \u001B[0;36mBaseTrainer.run\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    439\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m    440\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mis_run \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mTrue\u001B[39;00m\n\u001B[1;32m--> 441\u001B[0m     \u001B[43mdeque\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmaxlen\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m0\u001B[39;49m\u001B[43m)\u001B[49m  \u001B[38;5;66;03m# feed the entire iterator into a zero-length deque\u001B[39;00m\n\u001B[0;32m    442\u001B[0m     info \u001B[38;5;241m=\u001B[39m gather_info(\n\u001B[0;32m    443\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mstart_time, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtrain_collector, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtest_collector,\n\u001B[0;32m    444\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mbest_reward, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mbest_reward_std\n\u001B[0;32m    445\u001B[0m     )\n\u001B[0;32m    446\u001B[0m \u001B[38;5;28;01mfinally\u001B[39;00m:\n",
      "File \u001B[1;32mD:\\mambaforge\\envs\\rlenv\\lib\\site-packages\\tianshou\\trainer\\base.py:299\u001B[0m, in \u001B[0;36mBaseTrainer.__next__\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    296\u001B[0m         result[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mn/st\u001B[39m\u001B[38;5;124m\"\u001B[39m] \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mint\u001B[39m(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgradient_step)\n\u001B[0;32m    297\u001B[0m         t\u001B[38;5;241m.\u001B[39mupdate()\n\u001B[1;32m--> 299\u001B[0m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mpolicy_update_fn\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdata\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mresult\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    300\u001B[0m     t\u001B[38;5;241m.\u001B[39mset_postfix(\u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mdata)\n\u001B[0;32m    302\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m t\u001B[38;5;241m.\u001B[39mn \u001B[38;5;241m<\u001B[39m\u001B[38;5;241m=\u001B[39m t\u001B[38;5;241m.\u001B[39mtotal \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mstop_fn_flag:\n",
      "File \u001B[1;32mD:\\mambaforge\\envs\\rlenv\\lib\\site-packages\\tianshou\\trainer\\offpolicy.py:122\u001B[0m, in \u001B[0;36mOffpolicyTrainer.policy_update_fn\u001B[1;34m(self, data, result)\u001B[0m\n\u001B[0;32m    120\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m _ \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(\u001B[38;5;28mround\u001B[39m(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mupdate_per_step \u001B[38;5;241m*\u001B[39m result[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mn/st\u001B[39m\u001B[38;5;124m\"\u001B[39m])):\n\u001B[0;32m    121\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgradient_step \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1\u001B[39m\n\u001B[1;32m--> 122\u001B[0m     losses \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mpolicy\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mupdate\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbatch_size\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtrain_collector\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbuffer\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    123\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mlog_update_data(data, losses)\n",
      "File \u001B[1;32mD:\\mambaforge\\envs\\rlenv\\lib\\site-packages\\tianshou\\policy\\base.py:277\u001B[0m, in \u001B[0;36mBasePolicy.update\u001B[1;34m(self, sample_size, buffer, **kwargs)\u001B[0m\n\u001B[0;32m    275\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mupdating \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mTrue\u001B[39;00m\n\u001B[0;32m    276\u001B[0m batch \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mprocess_fn(batch, buffer, indices)\n\u001B[1;32m--> 277\u001B[0m result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mlearn(batch, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m    278\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mpost_process_fn(batch, buffer, indices)\n\u001B[0;32m    279\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mlr_scheduler \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n",
      "File \u001B[1;32mD:\\mambaforge\\envs\\rlenv\\lib\\site-packages\\tianshou\\policy\\modelfree\\discrete_sac.py:108\u001B[0m, in \u001B[0;36mDiscreteSACPolicy.learn\u001B[1;34m(self, batch, **kwargs)\u001B[0m\n\u001B[0;32m    103\u001B[0m act \u001B[38;5;241m=\u001B[39m to_torch(\n\u001B[0;32m    104\u001B[0m     batch\u001B[38;5;241m.\u001B[39mact[:, np\u001B[38;5;241m.\u001B[39mnewaxis], device\u001B[38;5;241m=\u001B[39mtarget_q\u001B[38;5;241m.\u001B[39mdevice, dtype\u001B[38;5;241m=\u001B[39mtorch\u001B[38;5;241m.\u001B[39mlong\n\u001B[0;32m    105\u001B[0m )\n\u001B[0;32m    107\u001B[0m \u001B[38;5;66;03m# critic 1\u001B[39;00m\n\u001B[1;32m--> 108\u001B[0m current_q1 \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcritic1\u001B[49m\u001B[43m(\u001B[49m\u001B[43mbatch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mobs\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgather\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m1\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mact\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241m.\u001B[39mflatten()\n\u001B[0;32m    109\u001B[0m td1 \u001B[38;5;241m=\u001B[39m current_q1 \u001B[38;5;241m-\u001B[39m target_q\n\u001B[0;32m    110\u001B[0m critic1_loss \u001B[38;5;241m=\u001B[39m (td1\u001B[38;5;241m.\u001B[39mpow(\u001B[38;5;241m2\u001B[39m) \u001B[38;5;241m*\u001B[39m weight)\u001B[38;5;241m.\u001B[39mmean()\n",
      "\u001B[1;31mRuntimeError\u001B[0m: index 1 is out of bounds for dimension 1 with size 1"
     ]
    }
   ],
   "source": [
    "ts.trainer.offpolicy_trainer(\n",
    "    dqn, train_collector, test_collector, max_epoch=10, batch_size=128,\n",
    "    step_per_epoch=10000, step_per_collect=10,\n",
    "    train_fn=train_fn,\n",
    "    episode_per_test=10, stop_fn=lambda rew: rew >= env.spec.reward_threshold,\n",
    "    logger=logger\n",
    ")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "outputs": [],
   "source": [
    "train_env.close()\n",
    "test_env.close()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
